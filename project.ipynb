{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7413c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions (full): 11495242\n",
      "Interactions after sampling: 223313\n",
      "Users after sampling: 50000\n",
      "Items after sampling: 27490\n",
      "Train shape: (173313, 20)\n",
      "Test shape: (31306, 20)\n",
      "Matrix shape: (31306, 22773)\n",
      "Final matrix shape: (31306, 22773)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82a8c9f6b2b4e42a17aedb0367eedaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully.\n",
      "\n",
      "===== ALS MODEL =====\n",
      "Recall@10: 0.05803999233373794\n",
      "Precision@10: 0.0058039992333736015\n",
      "\n",
      "===== POPULARITY BASELINE =====\n",
      "Recall@10: 0.1490129687599821\n",
      "Precision@10: 0.014901296875999412\n",
      "\n",
      "===== HYBRID MODEL =====\n",
      "Recall@10: 0.060755126812751546\n",
      "Precision@10: 0.006075512681274947\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_parquet(\"train.parquet\")\n",
    "\n",
    "print(\"Total interactions (full):\", len(df))\n",
    "\n",
    "\n",
    "\n",
    "sample_users = (\n",
    "    df[\"user_id\"]\n",
    "    .drop_duplicates()\n",
    "    .sample(50000, random_state=42)\n",
    ")\n",
    "\n",
    "df = df[df[\"user_id\"].isin(sample_users)]\n",
    "\n",
    "print(\"Interactions after sampling:\", len(df))\n",
    "print(\"Users after sampling:\", df[\"user_id\"].nunique())\n",
    "print(\"Items after sampling:\", df[\"product_id\"].nunique())\n",
    "\n",
    "\n",
    "df[\"interaction_weight\"] = df[\"event_type\"].map({\n",
    "    \"cart\": 1,\n",
    "    \"purchase\": 5\n",
    "})\n",
    "\n",
    "df[\"interaction_weight\"] = np.log1p(df[\"interaction_weight\"])\n",
    "\n",
    "\n",
    "\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in df.groupby(\"user_id\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "\n",
    "    group = group.sort_values(\"timestamp\")\n",
    "    train_list.append(group.iloc[:-1])\n",
    "    test_list.append(group.iloc[-1:])\n",
    "\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "user_ids = train_df[\"user_id\"].unique()\n",
    "product_ids = train_df[\"product_id\"].unique()\n",
    "\n",
    "user_to_index = {u: i for i, u in enumerate(user_ids)}\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "train_df[\"user_index\"] = train_df[\"user_id\"].map(user_to_index)\n",
    "train_df[\"product_index\"] = train_df[\"product_id\"].map(product_to_index)\n",
    "\n",
    "\n",
    "\n",
    "user_item_matrix = csr_matrix(\n",
    "    (\n",
    "        train_df[\"interaction_weight\"],\n",
    "        (train_df[\"user_index\"], train_df[\"product_index\"])\n",
    "    ),\n",
    "    shape=(len(user_to_index), len(product_to_index))\n",
    ").astype(\"float32\")\n",
    "\n",
    "print(\"Matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "\n",
    "nonzero_items = user_item_matrix.getnnz(axis=0) > 0\n",
    "user_item_matrix = user_item_matrix[:, nonzero_items]\n",
    "\n",
    "product_ids = product_ids[nonzero_items]\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "print(\"Final matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "\n",
    "alpha = 40\n",
    "confidence_matrix = user_item_matrix * alpha\n",
    "\n",
    "\n",
    "\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=32,\n",
    "    regularization=0.1,\n",
    "    iterations=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "als_model.fit(confidence_matrix)\n",
    "\n",
    "print(\"Model trained successfully.\")\n",
    "\n",
    "\n",
    "item_popularity = np.array(user_item_matrix.sum(axis=0)).ravel()\n",
    "popular_items = np.argsort(-item_popularity)\n",
    "\n",
    "def recommend_popular(k=10):\n",
    "    return [index_to_product[i] for i in popular_items[:k]]\n",
    "\n",
    "\n",
    "\n",
    "def recommend_als(user_id, k=10):\n",
    "\n",
    "    if user_id not in user_to_index:\n",
    "        return recommend_popular(k)\n",
    "\n",
    "    user_index = user_to_index[user_id]\n",
    "    user_row = user_item_matrix[user_index:user_index + 1]\n",
    "\n",
    "    item_indices, scores = als_model.recommend(\n",
    "        userid=user_index,\n",
    "        user_items=user_row,\n",
    "        N=k,\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "\n",
    "    return [index_to_product[i] for i in item_indices]\n",
    "\n",
    "\n",
    "\n",
    "def recommend_hybrid(user_id, k=10):\n",
    "\n",
    "    popular_recs = [index_to_product[i] for i in popular_items[:50]]\n",
    "\n",
    "    if user_id not in user_to_index:\n",
    "        return popular_recs[:k]\n",
    "\n",
    "    user_index = user_to_index[user_id]\n",
    "    user_row = user_item_matrix[user_index:user_index + 1]\n",
    "\n",
    "    item_indices, scores = als_model.recommend(\n",
    "        userid=user_index,\n",
    "        user_items=user_row,\n",
    "        N=50,\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "\n",
    "    if len(item_indices) == 0:\n",
    "        return popular_recs[:k]\n",
    "\n",
    "    # Normalize ALS scores\n",
    "    scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
    "\n",
    "    # Popularity scores\n",
    "    pop_scores = item_popularity[item_indices]\n",
    "    pop_scores = (pop_scores - pop_scores.min()) / (pop_scores.max() - pop_scores.min() + 1e-8)\n",
    "\n",
    "    # Weighted blend\n",
    "    final_scores = 0.6 * scores + 0.4 * pop_scores\n",
    "\n",
    "    top_idx = np.argsort(-final_scores)[:k]\n",
    "\n",
    "    return [index_to_product[item_indices[i]] for i in top_idx]\n",
    "\n",
    "\n",
    "\n",
    "def recall_at_k(model=\"hybrid\", k=10):\n",
    "\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for user_id, group in test_df.groupby(\"user_id\"):\n",
    "\n",
    "        actual_items = set(group[\"product_id\"])\n",
    "\n",
    "        if model == \"als\":\n",
    "            recommended = set(recommend_als(user_id, k))\n",
    "        elif model == \"popular\":\n",
    "            recommended = set(recommend_popular(k))\n",
    "        else:\n",
    "            recommended = set(recommend_hybrid(user_id, k))\n",
    "\n",
    "        hits += len(actual_items & recommended)\n",
    "        total += len(actual_items)\n",
    "\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def precision_at_k(model=\"hybrid\", k=10):\n",
    "\n",
    "    total_precision = 0\n",
    "    user_count = 0\n",
    "\n",
    "    for user_id, group in test_df.groupby(\"user_id\"):\n",
    "\n",
    "        actual_items = set(group[\"product_id\"])\n",
    "\n",
    "        if model == \"als\":\n",
    "            recommended = set(recommend_als(user_id, k))\n",
    "        elif model == \"popular\":\n",
    "            recommended = set(recommend_popular(k))\n",
    "        else:\n",
    "            recommended = set(recommend_hybrid(user_id, k))\n",
    "\n",
    "        total_precision += len(actual_items & recommended) / k\n",
    "        user_count += 1\n",
    "\n",
    "    return total_precision / user_count if user_count > 0 else 0\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n===== ALS MODEL =====\")\n",
    "print(\"Recall@10:\", recall_at_k(\"als\", 10))\n",
    "print(\"Precision@10:\", precision_at_k(\"als\", 10))\n",
    "\n",
    "print(\"\\n===== POPULARITY BASELINE =====\")\n",
    "print(\"Recall@10:\", recall_at_k(\"popular\", 10))\n",
    "print(\"Precision@10:\", precision_at_k(\"popular\", 10))\n",
    "\n",
    "print(\"\\n===== HYBRID MODEL =====\")\n",
    "print(\"Recall@10:\", recall_at_k(\"hybrid\", 10))\n",
    "print(\"Precision@10:\", precision_at_k(\"hybrid\", 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14ca462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(als_model, open(\"als_model.pkl\", \"wb\"))\n",
    "pickle.dump(user_item_matrix, open(\"user_item_matrix.pkl\", \"wb\"))\n",
    "pickle.dump(user_to_index, open(\"user_to_index.pkl\", \"wb\"))\n",
    "pickle.dump(index_to_product, open(\"index_to_product.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b89c5009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['110760953',\n",
       " '251333420',\n",
       " '310119844',\n",
       " '339009312',\n",
       " '402839293',\n",
       " '406827257',\n",
       " '407602302',\n",
       " '415514618',\n",
       " '415987845',\n",
       " '417810135']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(user_to_index.keys())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b8eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['event_time', 'event_type', 'product_id', 'brand', 'price', 'user_id',\n",
      "       'user_session', 'target', 'cat_0', 'cat_1', 'cat_2', 'cat_3',\n",
      "       'timestamp', 'ts_hour', 'ts_minute', 'ts_weekday', 'ts_day', 'ts_month',\n",
      "       'ts_year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"train.parquet\")\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d78983bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrain.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:258\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    257\u001b[39m     to_pandas_kwargs[\u001b[33m\"\u001b[39m\u001b[33msplit_blocks\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m path_or_handle, handles, filesystem = \u001b[43m_get_path_or_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    265\u001b[39m     pa_table = \u001b[38;5;28mself\u001b[39m.api.parquet.read_table(\n\u001b[32m    266\u001b[39m         path_or_handle,\n\u001b[32m    267\u001b[39m         columns=columns,\n\u001b[32m   (...)\u001b[39m\u001b[32m    270\u001b[39m         **kwargs,\n\u001b[32m    271\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parquet.py:141\u001b[39m, in \u001b[36m_get_path_or_handle\u001b[39m\u001b[34m(path, fs, storage_options, mode, is_dir)\u001b[39m\n\u001b[32m    131\u001b[39m handles = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    133\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m fs\n\u001b[32m    134\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dir\n\u001b[32m   (...)\u001b[39m\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# fsspec resources can also point to directories\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m     fs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    145\u001b[39m     path_or_handle = handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'train.parquet'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2f0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "102a6eaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m product_metadata = \u001b[43mtrain_df\u001b[49m.groupby(\u001b[33m\"\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m\"\u001b[39m).agg({\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mbrand\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mcat_0\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfirst\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprice\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m }).reset_index()\n\u001b[32m      7\u001b[39m product_metadata.to_csv(\u001b[33m\"\u001b[39m\u001b[33mproduct_metadata.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "product_metadata = train_df.groupby(\"product_id\").agg({\n",
    "    \"brand\": \"first\",\n",
    "    \"cat_0\": \"first\",\n",
    "    \"price\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "product_metadata.to_csv(\"product_metadata.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19d85c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.git',\n",
       " '.gitignore',\n",
       " 'als_model.pkl',\n",
       " 'app.py',\n",
       " 'index_to_item.pkl',\n",
       " 'index_to_product.pkl',\n",
       " 'popular_products.pkl',\n",
       " 'product_names.pkl',\n",
       " 'project.ipynb',\n",
       " 'README.md',\n",
       " 'recommender.py',\n",
       " 'requirements.txt',\n",
       " 'train_model.py',\n",
       " 'user_item_matrix.pkl',\n",
       " 'user_to_index.pkl',\n",
       " '__pycache__']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d99a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files created successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n",
    "\n",
    "\n",
    "train_df[\"price\"] = pd.to_numeric(train_df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "\n",
    "product_metadata = train_df.groupby(\"product_id\").agg({\n",
    "    \"brand\": \"first\",\n",
    "    \"cat_0\": \"first\",\n",
    "    \"price\": \"mean\"\n",
    "}).reset_index()\n",
    "\n",
    "product_metadata[\"price\"] = product_metadata[\"price\"].fillna(0)\n",
    "\n",
    "product_metadata.to_csv(\"product_metadata.csv\", index=False)\n",
    "\n",
    "purchase_counts = (\n",
    "    train_df[\"product_id\"]\n",
    "    .value_counts()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "purchase_counts.columns = [\"product_id\", \"purchase_count\"]\n",
    "\n",
    "purchase_counts.to_csv(\"product_popularity.csv\", index=False)\n",
    "\n",
    "print(\"Files created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15482c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata saved: 164453\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"train.parquet\")\n",
    "\n",
    "df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "product_metadata = (\n",
    "    df.groupby(\"product_id\")\n",
    "    .agg({\n",
    "        \"brand\": \"first\",\n",
    "        \"cat_0\": \"first\",\n",
    "        \"price\": \"mean\"\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "product_metadata[\"price\"] = product_metadata[\"price\"].fillna(0)\n",
    "\n",
    "product_metadata.to_csv(\"product_metadata.csv\", index=False)\n",
    "\n",
    "print(\"Metadata saved:\", len(product_metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "098738e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_parquet(\"train.parquet\")\n",
    "\n",
    "train_df[\"price\"] = pd.to_numeric(train_df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "product_metadata = (\n",
    "    train_df\n",
    "    .groupby(\"product_id\")\n",
    "    .agg({\n",
    "        \"brand\": \"first\",\n",
    "        \"cat_0\": \"first\",\n",
    "        \"price\": \"mean\"\n",
    "    })\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "product_metadata[\"price\"] = product_metadata[\"price\"].fillna(0)\n",
    "\n",
    "product_metadata.to_csv(\"product_metadata.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99692a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "product_popularity = (\n",
    "    train_df\n",
    "    .groupby(\"product_id\")\n",
    "    .size()\n",
    "    .reset_index(name=\"purchase_count\")\n",
    ")\n",
    "\n",
    "product_popularity.to_csv(\"product_popularity.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4e36e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "index_to_product = pickle.load(open(\"index_to_product.pkl\", \"rb\"))\n",
    "\n",
    "product_metadata = pd.read_csv(\"product_metadata.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4594f1f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "print(type(list(index_to_product.values())[0]))\n",
    "print(product_metadata[\"product_id\"].dtype)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
