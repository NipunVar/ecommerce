{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5048893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (17050, 20)\n",
      "Test shape: (3099, 20)\n",
      "Initial matrix shape: (3099, 4186)\n",
      "Final matrix shape: (3099, 4186)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n",
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\implicit\\utils.py:164: ParameterWarning: Method expects CSR input, and was passed coo_matrix instead. Converting to CSR took 0.0005974769592285156 seconds\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8566d282ed542ff8a51b7f6cfd2797f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model items: 4186\n",
      "Matrix items: 4186\n",
      "Recall@5 : 0.02968699580509842\n",
      "Recall@10: 0.03807679896740884\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. IMPORTS\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from implicit.nearest_neighbours import bm25_weight\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_parquet(\"train.parquet\")\n",
    "\n",
    "# Optional sampling for speed\n",
    "sample_users = (\n",
    "    df[\"user_id\"]\n",
    "    .drop_duplicates()\n",
    "    .sample(5000, random_state=42)\n",
    ")\n",
    "\n",
    "df = df[df[\"user_id\"].isin(sample_users)].copy()\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. INTERACTION WEIGHTING\n",
    "# =========================================================\n",
    "df[\"interaction_weight\"] = df[\"event_type\"].map({\n",
    "    \"cart\": 1,\n",
    "    \"purchase\": 5\n",
    "})\n",
    "\n",
    "# Log scaling\n",
    "df[\"interaction_weight\"] = np.log1p(df[\"interaction_weight\"])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. TRAIN / TEST SPLIT (Last Interaction)\n",
    "# =========================================================\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in df.groupby(\"user_id\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "\n",
    "    group = group.sort_values(\"timestamp\")\n",
    "    train_list.append(group.iloc[:-1])\n",
    "    test_list.append(group.iloc[-1:])\n",
    "\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. CREATE INDEX MAPPINGS\n",
    "# =========================================================\n",
    "user_ids = train_df[\"user_id\"].unique()\n",
    "product_ids = train_df[\"product_id\"].unique()\n",
    "\n",
    "user_to_index = {u: i for i, u in enumerate(user_ids)}\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "train_df[\"user_index\"] = train_df[\"user_id\"].map(user_to_index)\n",
    "train_df[\"product_index\"] = train_df[\"product_id\"].map(product_to_index)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. BUILD USER-ITEM MATRIX\n",
    "# =========================================================\n",
    "user_item_matrix = csr_matrix(\n",
    "    (\n",
    "        train_df[\"interaction_weight\"],\n",
    "        (train_df[\"user_index\"], train_df[\"product_index\"])\n",
    "    ),\n",
    "    shape=(len(user_to_index), len(product_to_index))\n",
    ").astype(\"double\")\n",
    "\n",
    "print(\"Initial matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. REMOVE EMPTY ITEM COLUMNS\n",
    "# =========================================================\n",
    "nonzero_items = user_item_matrix.getnnz(axis=0) > 0\n",
    "user_item_matrix = user_item_matrix[:, nonzero_items]\n",
    "\n",
    "# Update product mappings\n",
    "product_ids = product_ids[nonzero_items]\n",
    "\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "print(\"Final matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. APPLY BM25 WEIGHTING\n",
    "# =========================================================\n",
    "confidence_matrix = bm25_weight(user_item_matrix, K1=100, B=0.8)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. TRAIN ALS (OPTIMIZED HYPERPARAMETERS)\n",
    "# =========================================================\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=128,\n",
    "    regularization=0.01,\n",
    "    iterations=40,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "als_model.fit(confidence_matrix)\n",
    "\n",
    "print(\"Model items:\", als_model.item_factors.shape[0])\n",
    "print(\"Matrix items:\", user_item_matrix.shape[1])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. POPULARITY BACKUP (BOOST RECALL)\n",
    "# =========================================================\n",
    "item_popularity = np.array(user_item_matrix.sum(axis=0)).ravel()\n",
    "popular_items = np.argsort(-item_popularity)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11. RECOMMEND FUNCTION\n",
    "# =========================================================\n",
    "def recommend_als(user_id, n_products=10):\n",
    "\n",
    "    if user_id not in user_to_index:\n",
    "        return [index_to_product[i] for i in popular_items[:n_products]]\n",
    "\n",
    "    user_index = user_to_index[user_id]\n",
    "\n",
    "    if user_index >= user_item_matrix.shape[0]:\n",
    "        return [index_to_product[i] for i in popular_items[:n_products]]\n",
    "\n",
    "    user_row = user_item_matrix[user_index:user_index + 1]\n",
    "\n",
    "    item_indices, scores = als_model.recommend(\n",
    "        userid=user_index,\n",
    "        user_items=user_row,\n",
    "        N=n_products,\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "\n",
    "    # Backfill with popular items if needed\n",
    "    if len(item_indices) < n_products:\n",
    "        needed = n_products - len(item_indices)\n",
    "        item_indices = list(item_indices) + list(popular_items[:needed])\n",
    "\n",
    "    return [index_to_product[i] for i in item_indices]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12. RECALL@K EVALUATION\n",
    "# =========================================================\n",
    "def recall_at_k(k=10):\n",
    "\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for user_id, group in test_df.groupby(\"user_id\"):\n",
    "\n",
    "        if user_id not in user_to_index:\n",
    "            continue\n",
    "\n",
    "        actual_items = set(group[\"product_id\"])\n",
    "        recommended_items = set(recommend_als(user_id, n_products=k))\n",
    "\n",
    "        hits += len(actual_items & recommended_items)\n",
    "        total += len(actual_items)\n",
    "\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13. EVALUATE\n",
    "# =========================================================\n",
    "print(\"Recall@5 :\", recall_at_k(5))\n",
    "print(\"Recall@10:\", recall_at_k(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a103402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "169f9be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions: 11495242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     41\u001b[39m     train_list.append(group.iloc[:-\u001b[32m1\u001b[39m])\n\u001b[32m     42\u001b[39m     test_list.append(group.iloc[-\u001b[32m1\u001b[39m:])\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m train_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m test_df = pd.concat(test_list)\n\u001b[32m     47\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTrain shape:\u001b[39m\u001b[33m\"\u001b[39m, train_df.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[39m, in \u001b[36mconcat\u001b[39m\u001b[34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[32m    380\u001b[39m     copy = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m op = \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    389\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    390\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    392\u001b[39m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:449\u001b[39m, in \u001b[36m_Concatenator.__init__\u001b[39m\u001b[34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[39m\n\u001b[32m    447\u001b[39m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[32m    448\u001b[39m ndims = \u001b[38;5;28mself\u001b[39m._get_ndims(objs)\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m sample, objs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_sample_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;66;03m# Standardize axis parameter to int\u001b[39;00m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sample.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\concat.py:567\u001b[39m, in \u001b[36m_Concatenator._get_sample_object\u001b[39m\u001b[34m(self, objs, ndims, keys, names, levels)\u001b[39m\n\u001b[32m    562\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    565\u001b[39m     \u001b[38;5;66;03m# filter out the empties if we have not multi-index possibilities\u001b[39;00m\n\u001b[32m    566\u001b[39m     \u001b[38;5;66;03m# note to keep empty Series as it affect to result columns / name\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     non_empties = [obj \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m objs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m) > \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj.ndim == \u001b[32m1\u001b[39m]\n\u001b[32m    569\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(non_empties) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[32m    570\u001b[39m         keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m levels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.intersect\n\u001b[32m    571\u001b[39m     ):\n\u001b[32m    572\u001b[39m         objs = non_empties\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:1071\u001b[39m, in \u001b[36mDataFrame.shape\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1051\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshape\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m   1053\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1054\u001b[39m \u001b[33;03m    Return a tuple representing the dimensionality of the DataFrame.\u001b[39;00m\n\u001b[32m   1055\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1069\u001b[39m \u001b[33;03m    (2, 3)\u001b[39;00m\n\u001b[32m   1070\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1071\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.index), \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:912\u001b[39m, in \u001b[36mIndex.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    903\u001b[39m         c\n\u001b[32m    904\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unique(level=\u001b[32m0\u001b[39m)[: get_option(\u001b[33m\"\u001b[39m\u001b[33mdisplay.max_dir_items\u001b[39m\u001b[33m\"\u001b[39m)]\n\u001b[32m    905\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m c.isidentifier()\n\u001b[32m    906\u001b[39m     }\n\u001b[32m    908\u001b[39m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[32m    909\u001b[39m \u001b[38;5;66;03m# Array-Like Methods\u001b[39;00m\n\u001b[32m    910\u001b[39m \n\u001b[32m    911\u001b[39m \u001b[38;5;66;03m# ndarray compat\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m    913\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    914\u001b[39m \u001b[33;03m    Return the length of the Index.\u001b[39;00m\n\u001b[32m    915\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    916\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._data)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. IMPORTS\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. LOAD FULL DATASET (NO SAMPLING)\n",
    "# =========================================================\n",
    "df = pd.read_parquet(\"train.parquet\")\n",
    "\n",
    "print(\"Total interactions:\", len(df))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. INTERACTION WEIGHTING\n",
    "# =========================================================\n",
    "df[\"interaction_weight\"] = df[\"event_type\"].map({\n",
    "    \"cart\": 1,\n",
    "    \"purchase\": 5\n",
    "})\n",
    "\n",
    "# Log scaling\n",
    "df[\"interaction_weight\"] = np.log1p(df[\"interaction_weight\"])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. TRAIN / TEST SPLIT (LAST INTERACTION)\n",
    "# =========================================================\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in df.groupby(\"user_id\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "    \n",
    "    group = group.sort_values(\"timestamp\")\n",
    "    train_list.append(group.iloc[:-1])\n",
    "    test_list.append(group.iloc[-1:])\n",
    "\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. CREATE INDEX MAPPINGS\n",
    "# =========================================================\n",
    "user_ids = train_df[\"user_id\"].unique()\n",
    "product_ids = train_df[\"product_id\"].unique()\n",
    "\n",
    "user_to_index = {u: i for i, u in enumerate(user_ids)}\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "train_df[\"user_index\"] = train_df[\"user_id\"].map(user_to_index)\n",
    "train_df[\"product_index\"] = train_df[\"product_id\"].map(product_to_index)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. BUILD USER-ITEM MATRIX\n",
    "# =========================================================\n",
    "user_item_matrix = csr_matrix(\n",
    "    (\n",
    "        train_df[\"interaction_weight\"],\n",
    "        (train_df[\"user_index\"], train_df[\"product_index\"])\n",
    "    ),\n",
    "    shape=(len(user_to_index), len(product_to_index))\n",
    ").astype(\"double\")\n",
    "\n",
    "print(\"Initial matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. REMOVE EMPTY ITEM COLUMNS\n",
    "# =========================================================\n",
    "nonzero_items = user_item_matrix.getnnz(axis=0) > 0\n",
    "user_item_matrix = user_item_matrix[:, nonzero_items]\n",
    "\n",
    "# Update product mapping\n",
    "product_ids = product_ids[nonzero_items]\n",
    "\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "print(\"Final matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. CONFIDENCE SCALING (BETTER FOR SPARSE DATA)\n",
    "# =========================================================\n",
    "alpha = 50\n",
    "confidence_matrix = user_item_matrix * alpha\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. TRAIN ALS (STRONGER CONFIG)\n",
    "# =========================================================\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=128,\n",
    "    regularization=0.01,\n",
    "    iterations=40,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "als_model.fit(confidence_matrix)\n",
    "\n",
    "print(\"Model items:\", als_model.item_factors.shape[0])\n",
    "print(\"Matrix items:\", user_item_matrix.shape[1])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. POPULARITY SCORES (FOR HYBRID BLEND)\n",
    "# =========================================================\n",
    "item_popularity = np.array(user_item_matrix.sum(axis=0)).ravel()\n",
    "popular_items = np.argsort(-item_popularity)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11. HYBRID RECOMMEND FUNCTION (ALS + POPULARITY)\n",
    "# =========================================================\n",
    "def recommend_als(user_id, n_products=10):\n",
    "\n",
    "    if user_id not in user_to_index:\n",
    "        return [index_to_product[i] for i in popular_items[:n_products]]\n",
    "\n",
    "    user_index = user_to_index[user_id]\n",
    "\n",
    "    user_row = user_item_matrix[user_index:user_index + 1]\n",
    "\n",
    "    item_indices, scores = als_model.recommend(\n",
    "        userid=user_index,\n",
    "        user_items=user_row,\n",
    "        N=50,  # retrieve more candidates\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "\n",
    "    if len(item_indices) == 0:\n",
    "        return [index_to_product[i] for i in popular_items[:n_products]]\n",
    "\n",
    "    # Normalize ALS scores\n",
    "    scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
    "\n",
    "    # Popularity scores for same items\n",
    "    pop_scores = item_popularity[item_indices]\n",
    "    pop_scores = (pop_scores - pop_scores.min()) / (pop_scores.max() - pop_scores.min() + 1e-8)\n",
    "\n",
    "    # Blend scores\n",
    "    final_scores = 0.7 * scores + 0.3 * pop_scores\n",
    "\n",
    "    top_idx = np.argsort(-final_scores)[:n_products]\n",
    "\n",
    "    return [index_to_product[item_indices[i]] for i in top_idx]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12. RECALL@K\n",
    "# =========================================================\n",
    "def recall_at_k(k=10):\n",
    "\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for user_id, group in test_df.groupby(\"user_id\"):\n",
    "\n",
    "        if user_id not in user_to_index:\n",
    "            continue\n",
    "\n",
    "        actual_items = set(group[\"product_id\"])\n",
    "        recommended_items = set(recommend_als(user_id, n_products=k))\n",
    "\n",
    "        hits += len(actual_items & recommended_items)\n",
    "        total += len(actual_items)\n",
    "\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13. EVALUATE\n",
    "# =========================================================\n",
    "print(\"Recall@5 :\", recall_at_k(5))\n",
    "print(\"Recall@10:\", recall_at_k(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b373fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions (full): 11495242\n",
      "Interactions after sampling: 223313\n",
      "Users after sampling: 50000\n",
      "Items after sampling: 27490\n",
      "Train shape: (173313, 20)\n",
      "Test shape: (31306, 20)\n",
      "Matrix shape: (31306, 22773)\n",
      "Final matrix shape: (31306, 22773)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25623e7ecd9146c7add087b062210bff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully.\n",
      "Recall@5 : 0.03999233373794161\n",
      "Recall@10: 0.05803999233373794\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. IMPORTS\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_parquet(\"train.parquet\")\n",
    "\n",
    "print(\"Total interactions (full):\", len(df))\n",
    "\n",
    "# =========================================================\n",
    "# 3. SAMPLE 50K USERS (LAPTOP FRIENDLY)\n",
    "# =========================================================\n",
    "sample_users = (\n",
    "    df[\"user_id\"]\n",
    "    .drop_duplicates()\n",
    "    .sample(50000, random_state=42)\n",
    ")\n",
    "\n",
    "df = df[df[\"user_id\"].isin(sample_users)]\n",
    "\n",
    "print(\"Interactions after sampling:\", len(df))\n",
    "print(\"Users after sampling:\", df[\"user_id\"].nunique())\n",
    "print(\"Items after sampling:\", df[\"product_id\"].nunique())\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. INTERACTION WEIGHTING\n",
    "# =========================================================\n",
    "df[\"interaction_weight\"] = df[\"event_type\"].map({\n",
    "    \"cart\": 1,\n",
    "    \"purchase\": 5\n",
    "})\n",
    "\n",
    "df[\"interaction_weight\"] = np.log1p(df[\"interaction_weight\"])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. TRAIN / TEST SPLIT (LAST INTERACTION)\n",
    "# =========================================================\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in df.groupby(\"user_id\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "\n",
    "    group = group.sort_values(\"timestamp\")\n",
    "    train_list.append(group.iloc[:-1])\n",
    "    test_list.append(group.iloc[-1:])\n",
    "\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. CREATE INDEX MAPPINGS\n",
    "# =========================================================\n",
    "user_ids = train_df[\"user_id\"].unique()\n",
    "product_ids = train_df[\"product_id\"].unique()\n",
    "\n",
    "user_to_index = {u: i for i, u in enumerate(user_ids)}\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "train_df[\"user_index\"] = train_df[\"user_id\"].map(user_to_index)\n",
    "train_df[\"product_index\"] = train_df[\"product_id\"].map(product_to_index)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. BUILD USER-ITEM MATRIX\n",
    "# =========================================================\n",
    "user_item_matrix = csr_matrix(\n",
    "    (\n",
    "        train_df[\"interaction_weight\"],\n",
    "        (train_df[\"user_index\"], train_df[\"product_index\"])\n",
    "    ),\n",
    "    shape=(len(user_to_index), len(product_to_index))\n",
    ").astype(\"float32\")\n",
    "\n",
    "print(\"Matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. REMOVE EMPTY ITEM COLUMNS\n",
    "# =========================================================\n",
    "nonzero_items = user_item_matrix.getnnz(axis=0) > 0\n",
    "user_item_matrix = user_item_matrix[:, nonzero_items]\n",
    "\n",
    "product_ids = product_ids[nonzero_items]\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "print(\"Final matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. CONFIDENCE SCALING\n",
    "# =========================================================\n",
    "alpha = 40\n",
    "confidence_matrix = user_item_matrix * alpha\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. TRAIN ALS (FAST CONFIG)\n",
    "# =========================================================\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=32,        # reduced for speed\n",
    "    regularization=0.1,\n",
    "    iterations=10,     # fewer iterations\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "als_model.fit(confidence_matrix)\n",
    "\n",
    "print(\"Model trained successfully.\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11. POPULARITY BACKUP\n",
    "# =========================================================\n",
    "item_popularity = np.array(user_item_matrix.sum(axis=0)).ravel()\n",
    "popular_items = np.argsort(-item_popularity)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12. RECOMMEND FUNCTION\n",
    "# =========================================================\n",
    "def recommend_als(user_id, n_products=10):\n",
    "\n",
    "    if user_id not in user_to_index:\n",
    "        return [index_to_product[i] for i in popular_items[:n_products]]\n",
    "\n",
    "    user_index = user_to_index[user_id]\n",
    "    user_row = user_item_matrix[user_index:user_index + 1]\n",
    "\n",
    "    item_indices, scores = als_model.recommend(\n",
    "        userid=user_index,\n",
    "        user_items=user_row,\n",
    "        N=n_products,\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "\n",
    "    return [index_to_product[i] for i in item_indices]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13. RECALL@K\n",
    "# =========================================================\n",
    "def recall_at_k(k=10):\n",
    "\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for user_id, group in test_df.groupby(\"user_id\"):\n",
    "\n",
    "        if user_id not in user_to_index:\n",
    "            continue\n",
    "\n",
    "        actual_items = set(group[\"product_id\"])\n",
    "        recommended_items = set(recommend_als(user_id, n_products=k))\n",
    "\n",
    "        hits += len(actual_items & recommended_items)\n",
    "        total += len(actual_items)\n",
    "\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 14. EVALUATE\n",
    "# =========================================================\n",
    "print(\"Recall@5 :\", recall_at_k(5))\n",
    "print(\"Recall@10:\", recall_at_k(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b7413c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total interactions (full): 11495242\n",
      "Interactions after sampling: 223313\n",
      "Users after sampling: 50000\n",
      "Items after sampling: 27490\n",
      "Train shape: (173313, 20)\n",
      "Test shape: (31306, 20)\n",
      "Matrix shape: (31306, 22773)\n",
      "Final matrix shape: (31306, 22773)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\implicit\\cpu\\als.py:95: RuntimeWarning: OpenBLAS is configured to use 8 threads. It is highly recommended to disable its internal threadpool by setting the environment variable 'OPENBLAS_NUM_THREADS=1' or by calling 'threadpoolctl.threadpool_limits(1, \"blas\")'. Having OpenBLAS use a threadpool can lead to severe performance issues here.\n",
      "  check_blas_config()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82a8c9f6b2b4e42a17aedb0367eedaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model trained successfully.\n",
      "\n",
      "===== ALS MODEL =====\n",
      "Recall@10: 0.05803999233373794\n",
      "Precision@10: 0.0058039992333736015\n",
      "\n",
      "===== POPULARITY BASELINE =====\n",
      "Recall@10: 0.1490129687599821\n",
      "Precision@10: 0.014901296875999412\n",
      "\n",
      "===== HYBRID MODEL =====\n",
      "Recall@10: 0.060755126812751546\n",
      "Precision@10: 0.006075512681274947\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. IMPORTS\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 2. LOAD DATA\n",
    "# =========================================================\n",
    "df = pd.read_parquet(\"train.parquet\")\n",
    "\n",
    "print(\"Total interactions (full):\", len(df))\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 3. SAMPLE 50K USERS (FAST & STABLE)\n",
    "# =========================================================\n",
    "sample_users = (\n",
    "    df[\"user_id\"]\n",
    "    .drop_duplicates()\n",
    "    .sample(50000, random_state=42)\n",
    ")\n",
    "\n",
    "df = df[df[\"user_id\"].isin(sample_users)]\n",
    "\n",
    "print(\"Interactions after sampling:\", len(df))\n",
    "print(\"Users after sampling:\", df[\"user_id\"].nunique())\n",
    "print(\"Items after sampling:\", df[\"product_id\"].nunique())\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 4. INTERACTION WEIGHTING\n",
    "# =========================================================\n",
    "df[\"interaction_weight\"] = df[\"event_type\"].map({\n",
    "    \"cart\": 1,\n",
    "    \"purchase\": 5\n",
    "})\n",
    "\n",
    "df[\"interaction_weight\"] = np.log1p(df[\"interaction_weight\"])\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 5. TRAIN / TEST SPLIT (LAST INTERACTION)\n",
    "# =========================================================\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for user_id, group in df.groupby(\"user_id\"):\n",
    "    if len(group) < 2:\n",
    "        continue\n",
    "\n",
    "    group = group.sort_values(\"timestamp\")\n",
    "    train_list.append(group.iloc[:-1])\n",
    "    test_list.append(group.iloc[-1:])\n",
    "\n",
    "train_df = pd.concat(train_list)\n",
    "test_df = pd.concat(test_list)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Test shape:\", test_df.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 6. CREATE INDEX MAPPINGS\n",
    "# =========================================================\n",
    "user_ids = train_df[\"user_id\"].unique()\n",
    "product_ids = train_df[\"product_id\"].unique()\n",
    "\n",
    "user_to_index = {u: i for i, u in enumerate(user_ids)}\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "train_df[\"user_index\"] = train_df[\"user_id\"].map(user_to_index)\n",
    "train_df[\"product_index\"] = train_df[\"product_id\"].map(product_to_index)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. BUILD USER-ITEM MATRIX\n",
    "# =========================================================\n",
    "user_item_matrix = csr_matrix(\n",
    "    (\n",
    "        train_df[\"interaction_weight\"],\n",
    "        (train_df[\"user_index\"], train_df[\"product_index\"])\n",
    "    ),\n",
    "    shape=(len(user_to_index), len(product_to_index))\n",
    ").astype(\"float32\")\n",
    "\n",
    "print(\"Matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 8. REMOVE EMPTY ITEM COLUMNS\n",
    "# =========================================================\n",
    "nonzero_items = user_item_matrix.getnnz(axis=0) > 0\n",
    "user_item_matrix = user_item_matrix[:, nonzero_items]\n",
    "\n",
    "product_ids = product_ids[nonzero_items]\n",
    "product_to_index = {p: i for i, p in enumerate(product_ids)}\n",
    "index_to_product = {i: p for p, i in product_to_index.items()}\n",
    "\n",
    "print(\"Final matrix shape:\", user_item_matrix.shape)\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. CONFIDENCE SCALING\n",
    "# =========================================================\n",
    "alpha = 40\n",
    "confidence_matrix = user_item_matrix * alpha\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 10. TRAIN ALS MODEL\n",
    "# =========================================================\n",
    "als_model = AlternatingLeastSquares(\n",
    "    factors=32,\n",
    "    regularization=0.1,\n",
    "    iterations=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "als_model.fit(confidence_matrix)\n",
    "\n",
    "print(\"Model trained successfully.\")\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 11. POPULARITY BASELINE\n",
    "# =========================================================\n",
    "item_popularity = np.array(user_item_matrix.sum(axis=0)).ravel()\n",
    "popular_items = np.argsort(-item_popularity)\n",
    "\n",
    "def recommend_popular(k=10):\n",
    "    return [index_to_product[i] for i in popular_items[:k]]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 12. ALS RECOMMENDATION\n",
    "# =========================================================\n",
    "def recommend_als(user_id, k=10):\n",
    "\n",
    "    if user_id not in user_to_index:\n",
    "        return recommend_popular(k)\n",
    "\n",
    "    user_index = user_to_index[user_id]\n",
    "    user_row = user_item_matrix[user_index:user_index + 1]\n",
    "\n",
    "    item_indices, scores = als_model.recommend(\n",
    "        userid=user_index,\n",
    "        user_items=user_row,\n",
    "        N=k,\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "\n",
    "    return [index_to_product[i] for i in item_indices]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 13. HYBRID RECOMMENDATION (BEST PRACTICAL MODEL)\n",
    "# =========================================================\n",
    "def recommend_hybrid(user_id, k=10):\n",
    "\n",
    "    popular_recs = [index_to_product[i] for i in popular_items[:50]]\n",
    "\n",
    "    if user_id not in user_to_index:\n",
    "        return popular_recs[:k]\n",
    "\n",
    "    user_index = user_to_index[user_id]\n",
    "    user_row = user_item_matrix[user_index:user_index + 1]\n",
    "\n",
    "    item_indices, scores = als_model.recommend(\n",
    "        userid=user_index,\n",
    "        user_items=user_row,\n",
    "        N=50,\n",
    "        filter_already_liked_items=True\n",
    "    )\n",
    "\n",
    "    if len(item_indices) == 0:\n",
    "        return popular_recs[:k]\n",
    "\n",
    "    # Normalize ALS scores\n",
    "    scores = (scores - scores.min()) / (scores.max() - scores.min() + 1e-8)\n",
    "\n",
    "    # Popularity scores\n",
    "    pop_scores = item_popularity[item_indices]\n",
    "    pop_scores = (pop_scores - pop_scores.min()) / (pop_scores.max() - pop_scores.min() + 1e-8)\n",
    "\n",
    "    # Weighted blend\n",
    "    final_scores = 0.6 * scores + 0.4 * pop_scores\n",
    "\n",
    "    top_idx = np.argsort(-final_scores)[:k]\n",
    "\n",
    "    return [index_to_product[item_indices[i]] for i in top_idx]\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 14. EVALUATION METRICS\n",
    "# =========================================================\n",
    "def recall_at_k(model=\"hybrid\", k=10):\n",
    "\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for user_id, group in test_df.groupby(\"user_id\"):\n",
    "\n",
    "        actual_items = set(group[\"product_id\"])\n",
    "\n",
    "        if model == \"als\":\n",
    "            recommended = set(recommend_als(user_id, k))\n",
    "        elif model == \"popular\":\n",
    "            recommended = set(recommend_popular(k))\n",
    "        else:\n",
    "            recommended = set(recommend_hybrid(user_id, k))\n",
    "\n",
    "        hits += len(actual_items & recommended)\n",
    "        total += len(actual_items)\n",
    "\n",
    "    return hits / total if total > 0 else 0\n",
    "\n",
    "\n",
    "def precision_at_k(model=\"hybrid\", k=10):\n",
    "\n",
    "    total_precision = 0\n",
    "    user_count = 0\n",
    "\n",
    "    for user_id, group in test_df.groupby(\"user_id\"):\n",
    "\n",
    "        actual_items = set(group[\"product_id\"])\n",
    "\n",
    "        if model == \"als\":\n",
    "            recommended = set(recommend_als(user_id, k))\n",
    "        elif model == \"popular\":\n",
    "            recommended = set(recommend_popular(k))\n",
    "        else:\n",
    "            recommended = set(recommend_hybrid(user_id, k))\n",
    "\n",
    "        total_precision += len(actual_items & recommended) / k\n",
    "        user_count += 1\n",
    "\n",
    "    return total_precision / user_count if user_count > 0 else 0\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 15. FINAL RESULTS\n",
    "# =========================================================\n",
    "print(\"\\n===== ALS MODEL =====\")\n",
    "print(\"Recall@10:\", recall_at_k(\"als\", 10))\n",
    "print(\"Precision@10:\", precision_at_k(\"als\", 10))\n",
    "\n",
    "print(\"\\n===== POPULARITY BASELINE =====\")\n",
    "print(\"Recall@10:\", recall_at_k(\"popular\", 10))\n",
    "print(\"Precision@10:\", precision_at_k(\"popular\", 10))\n",
    "\n",
    "print(\"\\n===== HYBRID MODEL =====\")\n",
    "print(\"Recall@10:\", recall_at_k(\"hybrid\", 10))\n",
    "print(\"Precision@10:\", precision_at_k(\"hybrid\", 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b14ca462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(als_model, open(\"als_model.pkl\", \"wb\"))\n",
    "pickle.dump(user_item_matrix, open(\"user_item_matrix.pkl\", \"wb\"))\n",
    "pickle.dump(user_to_index, open(\"user_to_index.pkl\", \"wb\"))\n",
    "pickle.dump(index_to_product, open(\"index_to_product.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b89c5009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['110760953',\n",
       " '251333420',\n",
       " '310119844',\n",
       " '339009312',\n",
       " '402839293',\n",
       " '406827257',\n",
       " '407602302',\n",
       " '415514618',\n",
       " '415987845',\n",
       " '417810135']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(user_to_index.keys())[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9b8eafb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['event_time', 'event_type', 'product_id', 'brand', 'price', 'user_id',\n",
      "       'user_session', 'target', 'cat_0', 'cat_1', 'cat_2', 'cat_3',\n",
      "       'timestamp', 'ts_hour', 'ts_minute', 'ts_weekday', 'ts_day', 'ts_month',\n",
      "       'ts_year'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"train.parquet\")\n",
    "print(df.columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
